{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d7e894d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n═══════════════════════════════════════════════════════════════════════════════════════════════════════\\nEnhanced Synthetic Data Validation Suite v2.0\\nUrals Loading ML Forecasting\\n\\nStatistical tests for synthetic vs real data comparison\\nDate: November 2025\\n═══════════════════════════════════════════════════════════════════════════════════════════════════════\\n\\nOVERVIEW:\\n---------\\nThis module implements a comprehensive statistical validation framework for comparing\\nsynthetic data against real observable data. Essential for validating ML training data\\nin commodity forecasting applications.\\n\\nTESTS IMPLEMENTED:\\n------------------\\n1. Descriptive Statistics (mean, std, skewness, kurtosis, percentiles)\\n2. Kolmogorov-Smirnov Two-Sample Test (distribution comparison)\\n3. Anderson-Darling Test (tail-sensitive normality)\\n4. Jarque-Bera Test (combined skew/kurtosis normality)\\n5. D'Agostino-Pearson Test (normality)\\n6. Mann-Whitney U Test (non-parametric comparison)\\n7. Levene's Test (variance equality)\\n8. Correlation Structure Validation\\n9. Tail Risk Analysis (VaR percentiles)\\n\\n\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "Enhanced Synthetic Data Validation Suite v2.0\n",
    "Urals Loading ML Forecasting\n",
    "\n",
    "Statistical tests for synthetic vs real data comparison\n",
    "Date: November 2025\n",
    "═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "OVERVIEW:\n",
    "---------\n",
    "This module implements a comprehensive statistical validation framework for comparing\n",
    "synthetic data against real observable data. Essential for validating ML training data\n",
    "in commodity forecasting applications.\n",
    "\n",
    "TESTS IMPLEMENTED:\n",
    "------------------\n",
    "1. Descriptive Statistics (mean, std, skewness, kurtosis, percentiles)\n",
    "2. Kolmogorov-Smirnov Two-Sample Test (distribution comparison)\n",
    "3. Anderson-Darling Test (tail-sensitive normality)\n",
    "4. Jarque-Bera Test (combined skew/kurtosis normality)\n",
    "5. D'Agostino-Pearson Test (normality)\n",
    "6. Mann-Whitney U Test (non-parametric comparison)\n",
    "7. Levene's Test (variance equality)\n",
    "8. Correlation Structure Validation\n",
    "9. Tail Risk Analysis (VaR percentiles)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "20fb3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "21e70d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# CONFIGURATION\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration for statistical tests\"\"\"\n",
    "    ALPHA = 0.05  # Significance level\n",
    "    KS_THRESHOLD = 0.05\n",
    "    AD_CRITICAL = 0.787  # Anderson-Darling critical value at 5%\n",
    "    CORR_ERROR_EXCELLENT = 0.03\n",
    "    CORR_ERROR_GOOD = 0.05\n",
    "    CORR_ERROR_ACCEPTABLE = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ad1d2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 1: DESCRIPTIVE STATISTICS\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def compute_descriptive_stats(series: pd.Series, name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute comprehensive descriptive statistics for a series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Data series to analyze\n",
    "    name : str\n",
    "        Variable name for labeling\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all descriptive statistics\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'Variable': name,\n",
    "        'N': len(series),\n",
    "        'Mean': series.mean(),\n",
    "        'Std': series.std(),\n",
    "        'Min': series.min(),\n",
    "        'Q1': series.quantile(0.25),\n",
    "        'Median': series.median(),\n",
    "        'Q3': series.quantile(0.75),\n",
    "        'Max': series.max(),\n",
    "        'Range': series.max() - series.min(),\n",
    "        'IQR': series.quantile(0.75) - series.quantile(0.25),\n",
    "        'CV_pct': (series.std() / series.mean() * 100) if series.mean() != 0 else np.nan,\n",
    "        'Skewness': stats.skew(series),\n",
    "        'Kurtosis': stats.kurtosis(series),  # Excess kurtosis\n",
    "        'P1': series.quantile(0.01),\n",
    "        'P5': series.quantile(0.05),\n",
    "        'P95': series.quantile(0.95),\n",
    "        'P99': series.quantile(0.99)\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_descriptive_stats(real: pd.Series, synthetic: pd.Series, var_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare descriptive statistics between real and synthetic data.\n",
    "    \n",
    "    Returns percentage differences for key metrics.\n",
    "    \"\"\"\n",
    "    real_stats = compute_descriptive_stats(real, f\"Real_{var_name}\")\n",
    "    synth_stats = compute_descriptive_stats(synthetic, f\"Synth_{var_name}\")\n",
    "    \n",
    "    mean_diff_pct = abs(real_stats['Mean'] - synth_stats['Mean']) / abs(real_stats['Mean']) * 100\n",
    "    std_diff_pct = abs(real_stats['Std'] - synth_stats['Std']) / abs(real_stats['Std']) * 100\n",
    "    \n",
    "    return {\n",
    "        'Variable': var_name,\n",
    "        'Real_Mean': real_stats['Mean'],\n",
    "        'Synth_Mean': synth_stats['Mean'],\n",
    "        'Mean_Error_Pct': mean_diff_pct,\n",
    "        'Real_Std': real_stats['Std'],\n",
    "        'Synth_Std': synth_stats['Std'],\n",
    "        'Std_Error_Pct': std_diff_pct,\n",
    "        'Real_Skewness': real_stats['Skewness'],\n",
    "        'Synth_Skewness': synth_stats['Skewness'],\n",
    "        'Real_Kurtosis': real_stats['Kurtosis'],\n",
    "        'Synth_Kurtosis': synth_stats['Kurtosis']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "62a5e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 2: DISTRIBUTION SHAPE ANALYSIS (SKEWNESS & KURTOSIS)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def analyze_distribution_shape(series: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze distribution shape through skewness and kurtosis.\n",
    "    \n",
    "    Skewness interpretation:\n",
    "        |skew| < 0.5: Approximately symmetric\n",
    "        |skew| < 1.0: Moderate skew\n",
    "        |skew| > 1.0: Highly skewed\n",
    "    \n",
    "    Kurtosis interpretation (excess):\n",
    "        kurt ≈ 0: Normal tails (mesokurtic)\n",
    "        kurt > 0: Fat tails (leptokurtic) - more extreme events\n",
    "        kurt < 0: Thin tails (platykurtic) - fewer extreme events\n",
    "    \"\"\"\n",
    "    skewness = stats.skew(series)\n",
    "    kurtosis = stats.kurtosis(series)\n",
    "    \n",
    "    # Interpret skewness\n",
    "    if abs(skewness) < 0.5:\n",
    "        skew_interp = \"Symmetric\"\n",
    "    elif abs(skewness) < 1.0:\n",
    "        skew_interp = \"Moderate skew\"\n",
    "    else:\n",
    "        skew_interp = \"Highly skewed\"\n",
    "    \n",
    "    # Interpret kurtosis\n",
    "    if abs(kurtosis) < 1:\n",
    "        kurt_interp = \"Normal tails\"\n",
    "    elif kurtosis > 1:\n",
    "        kurt_interp = \"Fat tails (risk)\"\n",
    "    else:\n",
    "        kurt_interp = \"Thin tails\"\n",
    "    \n",
    "    return {\n",
    "        'Skewness': skewness,\n",
    "        'Skewness_Interpretation': skew_interp,\n",
    "        'Kurtosis': kurtosis,\n",
    "        'Kurtosis_Interpretation': kurt_interp\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "763503a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 3: KOLMOGOROV-SMIRNOV TWO-SAMPLE TEST\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def ks_two_sample_test(real: pd.Series, synthetic: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Kolmogorov-Smirnov two-sample test for distribution comparison.\n",
    "    \n",
    "    H0: Both samples come from the same distribution\n",
    "    H1: Samples come from different distributions\n",
    "    \n",
    "    The KS statistic measures the maximum distance between empirical CDFs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    real : pd.Series\n",
    "        Real data series\n",
    "    synthetic : pd.Series\n",
    "        Synthetic data series\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Test results including statistic, p-value, and interpretation\n",
    "    \"\"\"\n",
    "    ks_stat, p_value = stats.ks_2samp(real.dropna(), synthetic.dropna())\n",
    "    \n",
    "    passed = p_value > Config.ALPHA\n",
    "    \n",
    "    if p_value > 0.10:\n",
    "        quality = \"EXCELLENT\"\n",
    "    elif p_value > 0.05:\n",
    "        quality = \"GOOD\"\n",
    "    else:\n",
    "        quality = \"POOR\"\n",
    "    \n",
    "    return {\n",
    "        'Test': 'Kolmogorov-Smirnov',\n",
    "        'Statistic': ks_stat,\n",
    "        'p_value': p_value,\n",
    "        'Alpha': Config.ALPHA,\n",
    "        'Passed': passed,\n",
    "        'Quality': quality,\n",
    "        'Interpretation': f\"Distributions are {'statistically similar' if passed else 'significantly different'}\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "066da087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 4: ANDERSON-DARLING TEST (TAIL-SENSITIVE)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def anderson_darling_test(series: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Anderson-Darling test for normality.\n",
    "    \n",
    "    More sensitive to distribution tails than KS test.\n",
    "    Critical for VaR/CVaR risk modeling where tails matter.\n",
    "    \n",
    "    H0: Data follows normal distribution\n",
    "    H1: Data does not follow normal distribution\n",
    "    \"\"\"\n",
    "    # Standardize the data\n",
    "    standardized = (series - series.mean()) / series.std()\n",
    "    \n",
    "    result = stats.anderson(standardized, dist='norm')\n",
    "    \n",
    "    # Critical value at 5% significance (index 2)\n",
    "    critical_5pct = result.critical_values[2]\n",
    "    passed = result.statistic < critical_5pct\n",
    "    \n",
    "    return {\n",
    "        'Test': 'Anderson-Darling',\n",
    "        'Statistic': result.statistic,\n",
    "        'Critical_Value_5pct': critical_5pct,\n",
    "        'Passed': passed,\n",
    "        'Interpretation': f\"Data is {'approximately normal' if passed else 'non-normal'}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dd70d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 5: JARQUE-BERA TEST\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def jarque_bera_test(series: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Jarque-Bera test for normality.\n",
    "    \n",
    "    Combines skewness and kurtosis into a single test statistic.\n",
    "    JB = (n/6) * [S² + (K²/4)]\n",
    "    \n",
    "    Widely used in econometrics and finance.\n",
    "    \n",
    "    H0: Data follows normal distribution (S=0, K=0)\n",
    "    H1: Data does not follow normal distribution\n",
    "    \"\"\"\n",
    "    jb_stat, p_value = stats.jarque_bera(series.dropna())\n",
    "    \n",
    "    passed = p_value > Config.ALPHA\n",
    "    \n",
    "    return {\n",
    "        'Test': 'Jarque-Bera',\n",
    "        'Statistic': jb_stat,\n",
    "        'p_value': p_value,\n",
    "        'Passed': passed,\n",
    "        'Interpretation': f\"Data is {'approximately normal' if passed else 'non-normal'}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4c93de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 6: D'AGOSTINO-PEARSON TEST\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def dagostino_pearson_test(series: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    D'Agostino-Pearson test for normality.\n",
    "    \n",
    "    Tests whether skewness and kurtosis significantly deviate from normal.\n",
    "    Combines tests for skewness and kurtosis.\n",
    "    \n",
    "    H0: Data follows normal distribution\n",
    "    H1: Data does not follow normal distribution\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stat, p_value = stats.normaltest(series.dropna())\n",
    "        passed = p_value > Config.ALPHA\n",
    "        \n",
    "        return {\n",
    "            'Test': 'DAgostino-Pearson',\n",
    "            'Statistic': stat,\n",
    "            'p_value': p_value,\n",
    "            'Passed': passed,\n",
    "            'Interpretation': f\"Data is {'approximately normal' if passed else 'non-normal'}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'Test': 'DAgostino-Pearson',\n",
    "            'Error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7d35112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 7: MANN-WHITNEY U TEST (NON-PARAMETRIC)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def mann_whitney_test(real: pd.Series, synthetic: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Mann-Whitney U test (non-parametric).\n",
    "    \n",
    "    Does not assume normality - more robust when data is skewed.\n",
    "    Tests whether two samples have the same distribution.\n",
    "    \n",
    "    H0: Both samples have the same distribution\n",
    "    H1: Samples have different distributions\n",
    "    \"\"\"\n",
    "    u_stat, p_value = stats.mannwhitneyu(\n",
    "        real.dropna(), \n",
    "        synthetic.dropna(), \n",
    "        alternative='two-sided'\n",
    "    )\n",
    "    \n",
    "    passed = p_value > Config.ALPHA\n",
    "    \n",
    "    return {\n",
    "        'Test': 'Mann-Whitney U',\n",
    "        'Statistic': u_stat,\n",
    "        'p_value': p_value,\n",
    "        'Passed': passed,\n",
    "        'Interpretation': f\"Distributions are {'statistically similar' if passed else 'significantly different'}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b6254f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 8: LEVENE'S TEST FOR VARIANCE EQUALITY\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def levene_test(real: pd.Series, synthetic: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Levene's test for equality of variances.\n",
    "    \n",
    "    Critical for risk modeling where variance (volatility) drives metrics.\n",
    "    \n",
    "    H0: Both samples have equal variances\n",
    "    H1: Samples have different variances\n",
    "    \"\"\"\n",
    "    lev_stat, p_value = stats.levene(real.dropna(), synthetic.dropna())\n",
    "    \n",
    "    passed = p_value > Config.ALPHA\n",
    "    \n",
    "    return {\n",
    "        'Test': 'Levene',\n",
    "        'Statistic': lev_stat,\n",
    "        'p_value': p_value,\n",
    "        'Real_Variance': real.var(),\n",
    "        'Synthetic_Variance': synthetic.var(),\n",
    "        'Passed': passed,\n",
    "        'Interpretation': f\"Variances are {'equal' if passed else 'significantly different'}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e95a8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 9: CORRELATION STRUCTURE VALIDATION\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def validate_correlation_structure(\n",
    "    real_df: pd.DataFrame, \n",
    "    synthetic_df: pd.DataFrame,\n",
    "    var_names: List[str]\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Validate that correlation structure is preserved between real and synthetic data.\n",
    "    \n",
    "    Critical for:\n",
    "    - ML model training (relationships must be preserved)\n",
    "    - Hedging strategies (correlation drives hedge ratios)\n",
    "    - Portfolio optimization (covariance matrix)\n",
    "    \"\"\"\n",
    "    real_corr = real_df.corr()\n",
    "    synth_corr = synthetic_df.corr()\n",
    "    \n",
    "    # Compute error matrix\n",
    "    error_matrix = (real_corr - synth_corr).abs()\n",
    "    \n",
    "    # Extract pairwise comparisons\n",
    "    pairs = []\n",
    "    n = len(var_names)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            var1, var2 = var_names[i], var_names[j]\n",
    "            real_c = real_corr.iloc[i, j]\n",
    "            synth_c = synth_corr.iloc[i, j]\n",
    "            error = abs(real_c - synth_c)\n",
    "            error_pct = (error / abs(real_c) * 100) if real_c != 0 else np.inf\n",
    "            \n",
    "            # Quality rating\n",
    "            if error < Config.CORR_ERROR_EXCELLENT:\n",
    "                quality = \"EXCELLENT\"\n",
    "            elif error < Config.CORR_ERROR_GOOD:\n",
    "                quality = \"GOOD\"\n",
    "            elif error < Config.CORR_ERROR_ACCEPTABLE:\n",
    "                quality = \"ACCEPTABLE\"\n",
    "            else:\n",
    "                quality = \"POOR\"\n",
    "            \n",
    "            pairs.append({\n",
    "                'Pair': f\"{var1}-{var2}\",\n",
    "                'Real_Corr': real_c,\n",
    "                'Synth_Corr': synth_c,\n",
    "                'Error': error,\n",
    "                'Error_Pct': error_pct,\n",
    "                'Quality': quality\n",
    "            })\n",
    "    \n",
    "    # Summary statistics\n",
    "    errors = [p['Error'] for p in pairs]\n",
    "    \n",
    "    return {\n",
    "        'Real_Correlation_Matrix': real_corr,\n",
    "        'Synthetic_Correlation_Matrix': synth_corr,\n",
    "        'Error_Matrix': error_matrix,\n",
    "        'Pairwise_Comparison': pairs,\n",
    "        'Mean_Absolute_Error': np.mean(errors),\n",
    "        'Max_Absolute_Error': np.max(errors),\n",
    "        'Std_of_Errors': np.std(errors),\n",
    "        'Pairs_Excellent': sum(1 for p in pairs if p['Quality'] == 'EXCELLENT'),\n",
    "        'Pairs_Good': sum(1 for p in pairs if p['Quality'] == 'GOOD'),\n",
    "        'Pairs_Acceptable': sum(1 for p in pairs if p['Quality'] == 'ACCEPTABLE'),\n",
    "        'Pairs_Poor': sum(1 for p in pairs if p['Quality'] == 'POOR')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3a95f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 10: TAIL RISK ANALYSIS (VaR/CVaR)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def analyze_tail_risk(real: pd.Series, synthetic: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze tail risk preservation for VaR/CVaR modeling.\n",
    "    \n",
    "    Compares key percentiles that matter for risk management:\n",
    "    - 1st percentile (99% VaR)\n",
    "    - 5th percentile (95% VaR)\n",
    "    - 95th percentile\n",
    "    - 99th percentile\n",
    "    \"\"\"\n",
    "    percentiles = [0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]\n",
    "    \n",
    "    comparisons = []\n",
    "    for p in percentiles:\n",
    "        real_q = real.quantile(p)\n",
    "        synth_q = synthetic.quantile(p)\n",
    "        diff_pct = abs(real_q - synth_q) / abs(real_q) * 100 if real_q != 0 else 0\n",
    "        \n",
    "        comparisons.append({\n",
    "            'Percentile': f\"{p*100:.0f}%\",\n",
    "            'Real': real_q,\n",
    "            'Synthetic': synth_q,\n",
    "            'Diff_Pct': diff_pct\n",
    "        })\n",
    "    \n",
    "    # Calculate tail coverage ratios\n",
    "    real_iqr = real.quantile(0.75) - real.quantile(0.25)\n",
    "    synth_iqr = synthetic.quantile(0.75) - synthetic.quantile(0.25)\n",
    "    \n",
    "    real_tail_range = real.quantile(0.99) - real.quantile(0.01)\n",
    "    synth_tail_range = synthetic.quantile(0.99) - synthetic.quantile(0.01)\n",
    "    \n",
    "    return {\n",
    "        'Percentile_Comparison': comparisons,\n",
    "        'IQR_Coverage': synth_iqr / real_iqr * 100 if real_iqr != 0 else np.nan,\n",
    "        'Tail_Range_Coverage': synth_tail_range / real_tail_range * 100 if real_tail_range != 0 else np.nan,\n",
    "        'VaR_99_Real': real.quantile(0.01),\n",
    "        'VaR_99_Synthetic': synthetic.quantile(0.01),\n",
    "        'VaR_95_Real': real.quantile(0.05),\n",
    "        'VaR_95_Synthetic': synthetic.quantile(0.05)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eca4d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# COMPREHENSIVE VALIDATION SUITE\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class SyntheticDataValidator:\n",
    "    \"\"\"\n",
    "    Comprehensive validation suite for synthetic data.\n",
    "    \n",
    "    Usage:\n",
    "    ------\n",
    "    validator = SyntheticDataValidator(real_df, synthetic_df, column_mapping)\n",
    "    results = validator.run_all_tests()\n",
    "    validator.generate_report()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        real_df: pd.DataFrame, \n",
    "        synthetic_df: pd.DataFrame,\n",
    "        column_mapping: Dict[str, Tuple[str, str]]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize validator.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        real_df : pd.DataFrame\n",
    "            Real data\n",
    "        synthetic_df : pd.DataFrame\n",
    "            Synthetic data\n",
    "        column_mapping : dict\n",
    "            Maps variable names to (real_col, synth_col) tuples\n",
    "        \"\"\"\n",
    "        self.real_df = real_df\n",
    "        self.synthetic_df = synthetic_df\n",
    "        self.column_mapping = column_mapping\n",
    "        self.results = {}\n",
    "    \n",
    "    def run_all_tests(self) -> Dict:\n",
    "        \"\"\"Run all validation tests.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for var_name, (real_col, synth_col) in self.column_mapping.items():\n",
    "            real_series = self.real_df[real_col].dropna()\n",
    "            synth_series = self.synthetic_df[synth_col].dropna()\n",
    "            \n",
    "            results[var_name] = {\n",
    "                'Descriptive': compare_descriptive_stats(real_series, synth_series, var_name),\n",
    "                'Distribution_Shape_Real': analyze_distribution_shape(real_series),\n",
    "                'Distribution_Shape_Synth': analyze_distribution_shape(synth_series),\n",
    "                'KS_Test': ks_two_sample_test(real_series, synth_series),\n",
    "                'Mann_Whitney': mann_whitney_test(real_series, synth_series),\n",
    "                'Levene': levene_test(real_series, synth_series),\n",
    "                'Jarque_Bera_Real': jarque_bera_test(real_series),\n",
    "                'Jarque_Bera_Synth': jarque_bera_test(synth_series),\n",
    "                'Anderson_Darling_Real': anderson_darling_test(real_series),\n",
    "                'Anderson_Darling_Synth': anderson_darling_test(synth_series),\n",
    "                'DAgostino_Real': dagostino_pearson_test(real_series),\n",
    "                'DAgostino_Synth': dagostino_pearson_test(synth_series),\n",
    "                'Tail_Risk': analyze_tail_risk(real_series, synth_series)\n",
    "            }\n",
    "        \n",
    "        self.results = results\n",
    "        return results\n",
    "    \n",
    "    def get_summary_table(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate summary table for presentation.\"\"\"\n",
    "        rows = []\n",
    "        for var_name, tests in self.results.items():\n",
    "            row = {\n",
    "                'Variable': var_name,\n",
    "                'KS_Test': 'PASS' if tests['KS_Test']['Passed'] else 'FAIL',\n",
    "                'KS_pvalue': tests['KS_Test']['p_value'],\n",
    "                'Mann_Whitney': 'PASS' if tests['Mann_Whitney']['Passed'] else 'FAIL',\n",
    "                'MW_pvalue': tests['Mann_Whitney']['p_value'],\n",
    "                'Levene': 'PASS' if tests['Levene']['Passed'] else 'FAIL',\n",
    "                'Levene_pvalue': tests['Levene']['p_value'],\n",
    "                'JB_Synth': 'PASS' if tests['Jarque_Bera_Synth']['Passed'] else 'FAIL',\n",
    "                'Mean_Error_Pct': tests['Descriptive']['Mean_Error_Pct'],\n",
    "                'Std_Error_Pct': tests['Descriptive']['Std_Error_Pct']\n",
    "            }\n",
    "            rows.append(row)\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "    def calculate_overall_score(self) -> float:\n",
    "        \"\"\"Calculate overall validation score (0-10).\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        for var_name, tests in self.results.items():\n",
    "            var_score = 0\n",
    "            \n",
    "            # KS test (weight: 2)\n",
    "            if tests['KS_Test']['Passed']:\n",
    "                var_score += 2\n",
    "            elif tests['KS_Test']['p_value'] > 0.01:\n",
    "                var_score += 1\n",
    "            \n",
    "            # Mann-Whitney (weight: 1.5)\n",
    "            if tests['Mann_Whitney']['Passed']:\n",
    "                var_score += 1.5\n",
    "            \n",
    "            # Levene (weight: 1.5)\n",
    "            if tests['Levene']['Passed']:\n",
    "                var_score += 1.5\n",
    "            \n",
    "            # Mean error (weight: 2)\n",
    "            mean_err = tests['Descriptive']['Mean_Error_Pct']\n",
    "            if mean_err < 1:\n",
    "                var_score += 2\n",
    "            elif mean_err < 5:\n",
    "                var_score += 1.5\n",
    "            elif mean_err < 10:\n",
    "                var_score += 1\n",
    "            \n",
    "            # Std error (weight: 1.5)\n",
    "            std_err = tests['Descriptive']['Std_Error_Pct']\n",
    "            if std_err < 5:\n",
    "                var_score += 1.5\n",
    "            elif std_err < 10:\n",
    "                var_score += 1\n",
    "            \n",
    "            # Normalize to 10\n",
    "            var_score = var_score / 8.5 * 10\n",
    "            scores.append(var_score)\n",
    "        \n",
    "        return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "24dd3c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GS QUANTITATIVE STRATEGIES - SYNTHETIC DATA VALIDATION SUITE\n",
      "================================================================================\n",
      "\n",
      "SUMMARY TABLE:\n",
      "Variable KS_Test  KS_pvalue Mann_Whitney  MW_pvalue Levene  Levene_pvalue JB_Synth  Mean_Error_Pct  Std_Error_Pct\n",
      "   URALS    PASS   0.466606         PASS   0.777607   PASS       0.127501     PASS        0.501464       8.912962\n",
      "   BRENT    PASS   0.950188         PASS   0.975810   PASS       0.856867     PASS        0.601098       0.125665\n",
      "    MOEX    PASS   0.066274         PASS   0.391148   PASS       0.748713     PASS        0.479987       0.597316\n",
      " NWEMURL    FAIL   0.048194         PASS   0.534538   PASS       0.074544     PASS       10.027315       2.529534\n",
      "\n",
      "OVERALL VALIDATION SCORE: 9.0/10\n",
      "\n",
      "CORRELATION PRESERVATION:\n",
      "Mean Absolute Error: 0.0301\n",
      "Max Absolute Error: 0.0612\n",
      "Pairs EXCELLENT: 4/6\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# MAIN EXECUTION\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GS QUANTITATIVE STRATEGIES - SYNTHETIC DATA VALIDATION SUITE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load data\n",
    "    real_data = pd.read_csv('Data_ML.csv')\n",
    "    synthetic_data = pd.read_csv('urals_synthetic_data.csv')\n",
    "    \n",
    "    # Fix Urals loading parsing\n",
    "    real_data['Urals loading'] = real_data['Urals loading'].str.replace(',', '').astype(float)\n",
    "    \n",
    "    # Define column mapping\n",
    "    column_mapping = {\n",
    "        'URALS': ('Urals loading', 'Urals loading'),\n",
    "        'BRENT': ('LCOc1', 'LCOc1'),\n",
    "        'MOEX': ('.IMOEX', '.IMOEX'),\n",
    "        'NWEMURL': ('NWEMURLCRKMc1', 'NWEMURLCRMc1')\n",
    "    }\n",
    "    \n",
    "    # Initialize validator\n",
    "    validator = SyntheticDataValidator(real_data, synthetic_data, column_mapping)\n",
    "    \n",
    "    # Run all tests\n",
    "    results = validator.run_all_tests()\n",
    "    \n",
    "    # Get summary\n",
    "    summary = validator.get_summary_table()\n",
    "    print(\"\\nSUMMARY TABLE:\")\n",
    "    print(summary.to_string(index=False))\n",
    "    \n",
    "    # Calculate overall score\n",
    "    score = validator.calculate_overall_score()\n",
    "    print(f\"\\nOVERALL VALIDATION SCORE: {score:.1f}/10\")\n",
    "    \n",
    "    # Correlation validation\n",
    "    real_cols = ['Urals loading', 'LCOc1', '.IMOEX', 'NWEMURLCRKMc1']\n",
    "    synth_cols = ['Urals loading', 'LCOc1', '.IMOEX', 'NWEMURLCRMc1']\n",
    "    \n",
    "    corr_results = validate_correlation_structure(\n",
    "        real_data[real_cols],\n",
    "        synthetic_data[synth_cols],\n",
    "        ['URALS', 'BRENT', 'MOEX', 'NWEMURL']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCORRELATION PRESERVATION:\")\n",
    "    print(f\"Mean Absolute Error: {corr_results['Mean_Absolute_Error']:.4f}\")\n",
    "    print(f\"Max Absolute Error: {corr_results['Max_Absolute_Error']:.4f}\")\n",
    "    print(f\"Pairs EXCELLENT: {corr_results['Pairs_Excellent']}/{len(corr_results['Pairwise_Comparison'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4bbe7e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n═══════════════════════════════════════════════════════════════════════════════════════════════════════\\nVISUALIZATION MODULE\\n═══════════════════════════════════════════════════════════════════════════════════════════════════════\\n\\nUSAGE:\\n------\\npython GS_Synthetic_Validation_Charts.py\\n\\nOUTPUT:\\n-------\\n4 publication-quality PNG charts at 300 DPI:\\n1. GS_Chart_1_Distributions.png     - Distribution overlay with KS test results\\n2. GS_Chart_2_QQ_Plots.png          - Q-Q plots for normality assessment\\n3. GS_Chart_3_Tests_Heatmap.png     - Statistical tests results matrix\\n4. GS_Chart_4_Moments.png           - Moments comparison (mean, std, skew, kurt)\\n5. GS_Chart_5_Correlation_Heatmap.png - Correlation structure preservation\\n6. GS_Chart_6_Tail_Risk.png         - VaR percentile comparison\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "VISUALIZATION MODULE\n",
    "═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "USAGE:\n",
    "------\n",
    "python GS_Synthetic_Validation_Charts.py\n",
    "\n",
    "OUTPUT:\n",
    "-------\n",
    "4 publication-quality PNG charts at 300 DPI:\n",
    "1. GS_Chart_1_Distributions.png     - Distribution overlay with KS test results\n",
    "2. GS_Chart_2_QQ_Plots.png          - Q-Q plots for normality assessment\n",
    "3. GS_Chart_3_Tests_Heatmap.png     - Statistical tests results matrix\n",
    "4. GS_Chart_4_Moments.png           - Moments comparison (mean, std, skew, kurt)\n",
    "5. GS_Chart_5_Correlation_Heatmap.png - Correlation structure preservation\n",
    "6. GS_Chart_6_Tail_Risk.png         - VaR percentile comparison\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0598f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4db85d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSChartingStandards:\n",
    "   \n",
    "    COLORS = {\n",
    "        'real': '#1f77b4',           # Blue (real data)\n",
    "        'synthetic': '#ff7f0e',      # Orange (synthetic data)\n",
    "        'pass': '#2ca02c',           # Green (pass threshold)\n",
    "        'fail': '#d62728',           # Red (fail threshold)\n",
    "        'caution': '#ff9800',        # Amber (caution zone)\n",
    "        'neutral': '#7f7f7f',        # Gray (neutral)\n",
    "        'highlight': '#1f77b4'       # Highlight blue\n",
    "    }\n",
    "    \n",
    "    \n",
    "    FONT_SIZE = {\n",
    "        'title': 14,\n",
    "        'label': 11,\n",
    "        'tick': 10,\n",
    "        'legend': 10,\n",
    "        'annotation': 9\n",
    "    }\n",
    "    \n",
    "    FIGURE_DPI = 300  # Publication quality\n",
    "    FIGURE_FORMAT = 'png'\n",
    "    \n",
    "    @staticmethod\n",
    "    def configure():\n",
    "        \"\"\"Apply GS charting standards to matplotlib session\"\"\"\n",
    "        sns.set_style(\"darkgrid\")\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "        # Set matplotlib rcParams for consistency\n",
    "        plt.rcParams.update({\n",
    "            'font.family': 'sans-serif',\n",
    "            'font.sans-serif': ['Arial', 'Helvetica'],\n",
    "            'font.size': GSChartingStandards.FONT_SIZE['label'],\n",
    "            'axes.labelsize': GSChartingStandards.FONT_SIZE['label'],\n",
    "            'axes.titlesize': GSChartingStandards.FONT_SIZE['title'],\n",
    "            'axes.labelweight': 'bold',\n",
    "            'axes.titleweight': 'bold',\n",
    "            'xtick.labelsize': GSChartingStandards.FONT_SIZE['tick'],\n",
    "            'ytick.labelsize': GSChartingStandards.FONT_SIZE['tick'],\n",
    "            'legend.fontsize': GSChartingStandards.FONT_SIZE['legend'],\n",
    "            'axes.linewidth': 1.2,\n",
    "            'axes.edgecolor': '#333333',\n",
    "            'grid.linewidth': 0.8,\n",
    "            'grid.alpha': 0.3,\n",
    "            'lines.linewidth': 2.0,\n",
    "            'patch.linewidth': 0.5,\n",
    "            'figure.facecolor': 'white',\n",
    "            'axes.facecolor': '#f8f9fa',\n",
    "            'savefig.facecolor': 'white',\n",
    "            'savefig.edgecolor': 'white',\n",
    "            'savefig.dpi': GSChartingStandards.FIGURE_DPI\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "16b45b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# DATA LOADING & PREPARATION\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and prepare data for visualization\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    real_data = pd.read_csv('Data_ML.csv')\n",
    "    synthetic_data = pd.read_csv('urals_synthetic_data.csv')\n",
    "    \n",
    "    # Fix Urals loading (parse comma-separated format)\n",
    "    real_data['Urals loading'] = real_data['Urals loading'].str.replace(',', '').astype(float)\n",
    "    \n",
    "    print(\"✓ Data loaded successfully\\n\")\n",
    "    return real_data, synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "52e00b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# CHART 1: DISTRIBUTION OVERLAYS WITH KS TEST\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def create_distribution_overlays(real_data, synthetic_data):\n",
    "    \"\"\"\n",
    "    Create distribution overlay plots with KDE and histograms.\n",
    "    Tests: Kolmogorov-Smirnov test for distribution equivalence.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating Chart 1: Distribution Overlays with KS Test...\")\n",
    "    \n",
    "    columns_map = {\n",
    "        'URALS': ('Urals loading', 'Urals loading'),\n",
    "        'BRENT': ('LCOc1', 'LCOc1'),\n",
    "        'MOEX': ('.IMOEX', '.IMOEX'),\n",
    "        'NWEMURL': ('NWEMURLCRKMc1', 'NWEMURLCRMc1')\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Distribution Comparison: Real vs Synthetic Data\\nKolmogorov-Smirnov Test Results', \n",
    "                 fontsize=GSChartingStandards.FONT_SIZE['title'], fontweight='bold', y=0.995)\n",
    "    \n",
    "    for idx, (var_name, (real_col, synth_col)) in enumerate(columns_map.items()):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        real_vals = real_data[real_col].dropna()\n",
    "        synth_vals = synthetic_data[synth_col].dropna()\n",
    "        \n",
    "        # Kolmogorov-Smirnov test\n",
    "        ks_stat, ks_pval = stats.ks_2samp(real_vals, synth_vals)\n",
    "        \n",
    "        # Histograms with transparency\n",
    "        bins = 40\n",
    "        ax.hist(real_vals, bins=bins, alpha=0.5, color=GSChartingStandards.COLORS['real'], \n",
    "                label='Real Data', density=True, edgecolor='black', linewidth=0.5)\n",
    "        ax.hist(synth_vals, bins=bins, alpha=0.5, color=GSChartingStandards.COLORS['synthetic'], \n",
    "                label='Synthetic Data', density=True, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        # KDE curves\n",
    "        kde_real = gaussian_kde(real_vals)\n",
    "        kde_synth = gaussian_kde(synth_vals)\n",
    "        x_range = np.linspace(min(real_vals.min(), synth_vals.min()), \n",
    "                              max(real_vals.max(), synth_vals.max()), 200)\n",
    "        ax.plot(x_range, kde_real(x_range), color=GSChartingStandards.COLORS['real'], \n",
    "                linewidth=2.5, label='Real KDE', linestyle='-')\n",
    "        ax.plot(x_range, kde_synth(x_range), color=GSChartingStandards.COLORS['synthetic'], \n",
    "                linewidth=2.5, label='Synthetic KDE', linestyle='--')\n",
    "        \n",
    "        # Test result formatting\n",
    "        result_color = GSChartingStandards.COLORS['pass'] if ks_pval > 0.05 else GSChartingStandards.COLORS['fail']\n",
    "        result_text = f\"KS p={ks_pval:.3f} {'✓ PASS' if ks_pval > 0.05 else '✗ FAIL'}\"\n",
    "        \n",
    "        ax.set_title(f'{var_name}\\n{result_text}', \n",
    "                     fontsize=GSChartingStandards.FONT_SIZE['label'], \n",
    "                     fontweight='bold', color=result_color)\n",
    "        ax.set_xlabel('Value', fontsize=GSChartingStandards.FONT_SIZE['label'])\n",
    "        ax.set_ylabel('Density', fontsize=GSChartingStandards.FONT_SIZE['label'])\n",
    "        ax.legend(loc='upper right', fontsize=GSChartingStandards.FONT_SIZE['legend'])\n",
    "        ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'GS_Chart_1_Distributions.{GSChartingStandards.FIGURE_FORMAT}'\n",
    "    plt.savefig(filename, dpi=GSChartingStandards.FIGURE_DPI, bbox_inches='tight')\n",
    "    print(f\"✓ Saved: {filename}\\n\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "44f688c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# CHART 2: Q-Q PLOTS FOR NORMALITY\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def create_qq_plots(synthetic_data):\n",
    "    \"\"\"\n",
    "    Create Q-Q plots for normality assessment.\n",
    "    Tests: Jarque-Bera test for normality of synthetic data.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating Chart 2: Q-Q Plots for Normality Assessment...\")\n",
    "    \n",
    "    columns_map = {\n",
    "        'URALS': 'Urals loading',\n",
    "        'BRENT': 'LCOc1',\n",
    "        'MOEX': '.IMOEX',\n",
    "        'NWEMURL': 'NWEMURLCRMc1'\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Q-Q Plots: Assessment of Normality\\nSynthetic Data Alignment with Normal Distribution', \n",
    "                 fontsize=GSChartingStandards.FONT_SIZE['title'], fontweight='bold', y=0.995)\n",
    "    \n",
    "    for idx, (var_name, synth_col) in enumerate(columns_map.items()):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        synth_vals = synthetic_data[synth_col].dropna()\n",
    "        \n",
    "        # Q-Q plot\n",
    "        stats.probplot(synth_vals, dist=\"norm\", plot=ax)\n",
    "        \n",
    "        # Styling\n",
    "        ax.get_lines()[0].set_color(GSChartingStandards.COLORS['synthetic'])\n",
    "        ax.get_lines()[0].set_markersize(6)\n",
    "        ax.get_lines()[0].set_alpha(0.7)\n",
    "        ax.get_lines()[1].set_color(GSChartingStandards.COLORS['real'])\n",
    "        ax.get_lines()[1].set_linewidth(2.5)\n",
    "        \n",
    "        # Jarque-Bera test\n",
    "        jb_stat, jb_pval = stats.jarque_bera(synth_vals)\n",
    "        result_color = GSChartingStandards.COLORS['pass'] if jb_pval > 0.05 else GSChartingStandards.COLORS['caution']\n",
    "        \n",
    "        ax.set_title(f'{var_name} (Synthetic)\\nJarque-Bera p={jb_pval:.4f}', \n",
    "                     fontsize=GSChartingStandards.FONT_SIZE['label'], \n",
    "                     fontweight='bold', color=result_color)\n",
    "        ax.set_xlabel('Theoretical Quantiles', fontsize=GSChartingStandards.FONT_SIZE['label'])\n",
    "        ax.set_ylabel('Sample Quantiles', fontsize=GSChartingStandards.FONT_SIZE['label'])\n",
    "        ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'GS_Chart_2_QQ_Plots.{GSChartingStandards.FIGURE_FORMAT}'\n",
    "    plt.savefig(filename, dpi=GSChartingStandards.FIGURE_DPI, bbox_inches='tight')\n",
    "    print(f\"✓ Saved: {filename}\\n\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c3328cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# CHART 3: STATISTICAL TESTS HEATMAP\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def create_tests_heatmap(real_data, synthetic_data):\n",
    "    \"\"\"\n",
    "    Create heatmap of statistical test results.\n",
    "    Tests: KS, Mann-Whitney, Levene, Jarque-Bera\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating Chart 3: Statistical Tests Heatmap...\")\n",
    "    \n",
    "    columns_map = {\n",
    "        'URALS': ('Urals loading', 'Urals loading'),\n",
    "        'BRENT': ('LCOc1', 'LCOc1'),\n",
    "        'MOEX': ('.IMOEX', '.IMOEX'),\n",
    "        'NWEMURL': ('NWEMURLCRKMc1', 'NWEMURLCRMc1')\n",
    "    }\n",
    "    \n",
    "    # Compute all test results\n",
    "    test_results = []\n",
    "    for var_name, (real_col, synth_col) in columns_map.items():\n",
    "        real_vals = real_data[real_col].dropna()\n",
    "        synth_vals = synthetic_data[synth_col].dropna()\n",
    "        \n",
    "        ks_stat, ks_pval = stats.ks_2samp(real_vals, synth_vals)\n",
    "        u_stat, u_pval = stats.mannwhitneyu(real_vals, synth_vals, alternative='two-sided')\n",
    "        lev_stat, lev_pval = stats.levene(real_vals, synth_vals)\n",
    "        jb_stat, jb_pval = stats.jarque_bera(synth_vals)\n",
    "        \n",
    "        test_results.append({\n",
    "            'Variable': var_name,\n",
    "            'KS Test': 1 if ks_pval > 0.05 else 0,\n",
    "            'Mann-Whitney': 1 if u_pval > 0.05 else 0,\n",
    "            'Levene': 1 if lev_pval > 0.05 else 0,\n",
    "            'Jarque-Bera': 1 if jb_pval > 0.05 else 0\n",
    "        })\n",
    "    \n",
    "    test_df = pd.DataFrame(test_results).set_index('Variable')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Heatmap\n",
    "    im = ax.imshow(test_df.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks(np.arange(len(test_df.columns)))\n",
    "    ax.set_yticks(np.arange(len(test_df.index)))\n",
    "    ax.set_xticklabels(test_df.columns, fontsize=GSChartingStandards.FONT_SIZE['label'])\n",
    "    ax.set_yticklabels(test_df.index, fontsize=GSChartingStandards.FONT_SIZE['label'])\n",
    "    \n",
    "    # Rotate x labels\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(test_df.index)):\n",
    "        for j in range(len(test_df.columns)):\n",
    "            value = test_df.values[i, j]\n",
    "            text = ax.text(j, i, '✓' if value == 1 else '✗',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\", \n",
    "                          fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.set_title('Statistical Tests Results Matrix\\nGreen=PASS (p>0.05), Red=FAIL (p<0.05)', \n",
    "                 fontsize=GSChartingStandards.FONT_SIZE['title'], \n",
    "                 fontweight='bold', pad=20)\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, orientation='vertical', pad=0.02)\n",
    "    cbar.set_label('Test Result', fontsize=GSChartingStandards.FONT_SIZE['label'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'GS_Chart_3_Tests_Heatmap.{GSChartingStandards.FIGURE_FORMAT}'\n",
    "    plt.savefig(filename, dpi=GSChartingStandards.FIGURE_DPI, bbox_inches='tight')\n",
    "    print(f\"✓ Saved: {filename}\\n\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0e7e80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# CHART 4: MOMENTS COMPARISON\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def create_moments_comparison(real_data, synthetic_data):\n",
    "    \"\"\"\n",
    "    Compare statistical moments (mean, std, skewness, kurtosis).\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating Chart 4: Moments Comparison...\")\n",
    "    \n",
    "    columns_map = {\n",
    "        'URALS': ('Urals loading', 'Urals loading'),\n",
    "        'BRENT': ('LCOc1', 'LCOc1'),\n",
    "        'MOEX': ('.IMOEX', '.IMOEX'),\n",
    "        'NWEMURL': ('NWEMURLCRKMc1', 'NWEMURLCRMc1')\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Statistical Moments Comparison: Real vs Synthetic\\nMean, Std Dev, Skewness, Kurtosis', \n",
    "                 fontsize=GSChartingStandards.FONT_SIZE['title'], fontweight='bold', y=0.995)\n",
    "    \n",
    "    moments_list = ['Mean', 'Std Dev', 'Skewness', 'Kurtosis']\n",
    "    moment_funcs = [\n",
    "        lambda x: x.mean(),\n",
    "        lambda x: x.std(),\n",
    "        lambda x: stats.skew(x),\n",
    "        lambda x: stats.kurtosis(x)\n",
    "    ]\n",
    "    \n",
    "    for idx, (moment_name, moment_func) in enumerate(zip(moments_list, moment_funcs)):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        real_moments = []\n",
    "        synth_moments = []\n",
    "        var_names = []\n",
    "        \n",
    "        for var_name, (real_col, synth_col) in columns_map.items():\n",
    "            real_vals = real_data[real_col].dropna()\n",
    "            synth_vals = synthetic_data[synth_col].dropna()\n",
    "            \n",
    "            real_moments.append(moment_func(real_vals))\n",
    "            synth_moments.append(moment_func(synth_vals))\n",
    "            var_names.append(var_name)\n",
    "        \n",
    "        # Bar plot\n",
    "        x = np.arange(len(var_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, real_moments, width, label='Real', \n",
    "                       color=GSChartingStandards.COLORS['real'], \n",
    "                       alpha=0.8, edgecolor='black', linewidth=1)\n",
    "        bars2 = ax.bar(x + width/2, synth_moments, width, label='Synthetic', \n",
    "                       color=GSChartingStandards.COLORS['synthetic'], \n",
    "                       alpha=0.8, edgecolor='black', linewidth=1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.2f}',\n",
    "                       ha='center', va='bottom', \n",
    "                       fontsize=GSChartingStandards.FONT_SIZE['annotation'])\n",
    "        \n",
    "        ax.set_xlabel('Variable', fontsize=GSChartingStandards.FONT_SIZE['label'])\n",
    "        ax.set_ylabel(moment_name, fontsize=GSChartingStandards.FONT_SIZE['label'])\n",
    "        ax.set_title(moment_name, fontsize=GSChartingStandards.FONT_SIZE['label'], fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(var_names)\n",
    "        ax.legend(fontsize=GSChartingStandards.FONT_SIZE['legend'])\n",
    "        ax.grid(True, alpha=0.3, axis='y', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'GS_Chart_4_Moments.{GSChartingStandards.FIGURE_FORMAT}'\n",
    "    plt.savefig(filename, dpi=GSChartingStandards.FIGURE_DPI, bbox_inches='tight')\n",
    "    print(f\"✓ Saved: {filename}\\n\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c9cf7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# CHART 5: CORRELATION STRUCTURE HEATMAP\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def create_correlation_heatmap(real_data, synthetic_data):\n",
    "    \"\"\"\n",
    "    Compare correlation structures (real vs synthetic).\n",
    "    Critical for ML training validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating Chart 5: Correlation Structure Heatmap...\")\n",
    "    \n",
    "    real_corr_cols = ['Urals loading', 'LCOc1', '.IMOEX', 'NWEMURLCRKMc1']\n",
    "    synth_corr_cols = ['Urals loading', 'LCOc1', '.IMOEX', 'NWEMURLCRMc1']\n",
    "    \n",
    "    real_corr = real_data[real_corr_cols].corr()\n",
    "    synth_corr = synthetic_data[synth_corr_cols].corr()\n",
    "    \n",
    "    # Rename for display\n",
    "    display_names = ['URALS', 'BRENT', 'MOEX', 'NWEMURL']\n",
    "    real_corr.index = display_names\n",
    "    real_corr.columns = display_names\n",
    "    synth_corr.index = display_names\n",
    "    synth_corr.columns = display_names\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    fig.suptitle('Correlation Structure: Real vs Synthetic with Error Matrix', \n",
    "                 fontsize=GSChartingStandards.FONT_SIZE['title'], fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Real correlation\n",
    "    sns.heatmap(real_corr, annot=True, fmt='.3f', cmap='coolwarm', center=0, \n",
    "                vmin=-1, vmax=1, ax=axes[0], cbar_kws={'label': 'Correlation'})\n",
    "    axes[0].set_title('Real Data', fontsize=GSChartingStandards.FONT_SIZE['label'], fontweight='bold')\n",
    "    \n",
    "    # Synthetic correlation\n",
    "    sns.heatmap(synth_corr, annot=True, fmt='.3f', cmap='coolwarm', center=0, \n",
    "                vmin=-1, vmax=1, ax=axes[1], cbar_kws={'label': 'Correlation'})\n",
    "    axes[1].set_title('Synthetic Data', fontsize=GSChartingStandards.FONT_SIZE['label'], fontweight='bold')\n",
    "    \n",
    "    # Error matrix\n",
    "    corr_error = (real_corr - synth_corr).abs()\n",
    "    sns.heatmap(corr_error, annot=True, fmt='.4f', cmap='Reds', \n",
    "                vmin=0, vmax=0.1, ax=axes[2], cbar_kws={'label': 'Absolute Error'})\n",
    "    axes[2].set_title('Correlation Error |Real - Synthetic|', \n",
    "                     fontsize=GSChartingStandards.FONT_SIZE['label'], fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'GS_Chart_5_Correlation_Heatmap.{GSChartingStandards.FIGURE_FORMAT}'\n",
    "    plt.savefig(filename, dpi=GSChartingStandards.FIGURE_DPI, bbox_inches='tight')\n",
    "    print(f\"✓ Saved: {filename}\\n\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "28f23dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# CHART 6: TAIL RISK ANALYSIS\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def create_tail_risk_analysis(real_data, synthetic_data):\n",
    "    \"\"\"\n",
    "    Compare tail risk (VaR percentiles).\n",
    "    Critical for risk management validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating Chart 6: Tail Risk Analysis (VaR)...\")\n",
    "    \n",
    "    columns_map = {\n",
    "        'URALS': ('Urals loading', 'Urals loading'),\n",
    "        'BRENT': ('LCOc1', 'LCOc1'),\n",
    "        'MOEX': ('.IMOEX', '.IMOEX')\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle('Tail Risk Analysis: VaR Percentile Comparison\\n1%, 5%, 95%, 99% Quantiles', \n",
    "                 fontsize=GSChartingStandards.FONT_SIZE['title'], fontweight='bold', y=0.98)\n",
    "    \n",
    "    percentiles = [0.01, 0.05, 0.95, 0.99]\n",
    "    percentile_labels = ['1% (99% VaR)', '5% (95% VaR)', '95%', '99%']\n",
    "    \n",
    "    for idx, (var_name, (real_col, synth_col)) in enumerate(columns_map.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        real_vals = real_data[real_col].dropna()\n",
    "        synth_vals = synthetic_data[synth_col].dropna()\n",
    "        \n",
    "        real_quantiles = [real_vals.quantile(p) for p in percentiles]\n",
    "        synth_quantiles = [synth_vals.quantile(p) for p in percentiles]\n",
    "        \n",
    "        # Bar plot\n",
    "        x = np.arange(len(percentile_labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, real_quantiles, width, label='Real', \n",
    "                       color=GSChartingStandards.COLORS['real'], \n",
    "                       alpha=0.8, edgecolor='black', linewidth=1)\n",
    "        bars2 = ax.bar(x + width/2, synth_quantiles, width, label='Synthetic', \n",
    "                       color=GSChartingStandards.COLORS['synthetic'], \n",
    "                       alpha=0.8, edgecolor='black', linewidth=1)\n",
    "        \n",
    "        # Add error percentages\n",
    "        for i, (r, s) in enumerate(zip(real_quantiles, synth_quantiles)):\n",
    "            error_pct = abs(r - s) / abs(r) * 100 if r != 0 else 0\n",
    "            ax.text(i, max(r, s) * 1.05, f'{error_pct:.1f}%',\n",
    "                   ha='center', va='bottom', fontsize=GSChartingStandards.FONT_SIZE['annotation'],\n",
    "                   fontweight='bold', color='darkred')\n",
    "        \n",
    "        ax.set_xlabel('Percentile', fontsize=GSChartingStandards.FONT_SIZE['label'])\n",
    "        ax.set_ylabel(f'{var_name} Value', fontsize=GSChartingStandards.FONT_SIZE['label'])\n",
    "        ax.set_title(f'{var_name}', fontsize=GSChartingStandards.FONT_SIZE['label'], fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(percentile_labels, fontsize=GSChartingStandards.FONT_SIZE['tick'])\n",
    "        ax.legend(fontsize=GSChartingStandards.FONT_SIZE['legend'])\n",
    "        ax.grid(True, alpha=0.3, axis='y', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'GS_Chart_6_Tail_Risk.{GSChartingStandards.FIGURE_FORMAT}'\n",
    "    plt.savefig(filename, dpi=GSChartingStandards.FIGURE_DPI, bbox_inches='tight')\n",
    "    print(f\"✓ Saved: {filename}\\n\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9fcf64ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUANTITATIVE STRATEGIES - SYNTHETIC DATA VALIDATION CHARTS\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "✓ Data loaded successfully\n",
      "\n",
      "Creating Chart 1: Distribution Overlays with KS Test...\n",
      "✓ Saved: GS_Chart_1_Distributions.png\n",
      "\n",
      "Creating Chart 2: Q-Q Plots for Normality Assessment...\n",
      "✓ Saved: GS_Chart_2_QQ_Plots.png\n",
      "\n",
      "Creating Chart 3: Statistical Tests Heatmap...\n",
      "✓ Saved: GS_Chart_3_Tests_Heatmap.png\n",
      "\n",
      "Creating Chart 4: Moments Comparison...\n",
      "✓ Saved: GS_Chart_4_Moments.png\n",
      "\n",
      "Creating Chart 5: Correlation Structure Heatmap...\n",
      "✓ Saved: GS_Chart_5_Correlation_Heatmap.png\n",
      "\n",
      "Creating Chart 6: Tail Risk Analysis (VaR)...\n",
      "✓ Saved: GS_Chart_6_Tail_Risk.png\n",
      "\n",
      "================================================================================\n",
      "SUCCESS: All 6 professional charts created\n",
      "================================================================================\n",
      "\n",
      "Output files:\n",
      "  1. GS_Chart_1_Distributions.png\n",
      "  2. GS_Chart_2_QQ_Plots.png\n",
      "  3. GS_Chart_3_Tests_Heatmap.png\n",
      "  4. GS_Chart_4_Moments.png\n",
      "  5. GS_Chart_5_Correlation_Heatmap.png\n",
      "  6. GS_Chart_6_Tail_Risk.png\n",
      "\n",
      "Format: PNG at 300 DPI (publication quality)\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "# MAIN EXECUTION\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def main():\n",
    "    \"\"\"Execute full visualization suite\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUANTITATIVE STRATEGIES - SYNTHETIC DATA VALIDATION CHARTS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Configure charting standards\n",
    "    GSChartingStandards.configure()\n",
    "    \n",
    "    # Load data\n",
    "    real_data, synthetic_data = load_data()\n",
    "    \n",
    "    # Create all charts\n",
    "    create_distribution_overlays(real_data, synthetic_data)\n",
    "    create_qq_plots(synthetic_data)\n",
    "    create_tests_heatmap(real_data, synthetic_data)\n",
    "    create_moments_comparison(real_data, synthetic_data)\n",
    "    create_correlation_heatmap(real_data, synthetic_data)\n",
    "    create_tail_risk_analysis(real_data, synthetic_data)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SUCCESS: All 6 professional charts created\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nOutput files:\")\n",
    "    print(\"  1. GS_Chart_1_Distributions.png\")\n",
    "    print(\"  2. GS_Chart_2_QQ_Plots.png\")\n",
    "    print(\"  3. GS_Chart_3_Tests_Heatmap.png\")\n",
    "    print(\"  4. GS_Chart_4_Moments.png\")\n",
    "    print(\"  5. GS_Chart_5_Correlation_Heatmap.png\")\n",
    "    print(\"  6. GS_Chart_6_Tail_Risk.png\")\n",
    "    print(f\"\\nFormat: PNG at {GSChartingStandards.FIGURE_DPI} DPI (publication quality)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
